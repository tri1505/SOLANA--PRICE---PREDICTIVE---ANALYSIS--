# -*- coding: utf-8 -*-
"""Submission 1 - analisis prediksi harga solana .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2GXCBQMdI1wpz8Q6mYbPNjtn0hUn9i1

##**Import Library**##

Import beberapa library yang diperlukan untuk analisa data, visualisasi data dan melatih model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import xgboost as xgb
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import GridSearchCV

"""##**Data Loading**##"""

data_path = '/content/SOL-USD.csv'

df = pd.read_csv(data_path)
df

"""#**Cek Informasi Pada Data**#"""

df.info()

"""Dapat dilihat bahwa tidak terdapat missing value pada dataset"""

df.describe()

"""Dapat dilihat pada visualisasi data dibawah. Terdapat banyak data outlier pada numerik yaitu price, open, high, low."""

numeric_features = df.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(15, 8))

for i, col in enumerate(numeric_features):
  plt.subplot(3,3,i+1)
  df.boxplot(column=col)

"""menerapkan teknik IQR Method yaitu dengan menghapus data yang berada diluar interquartile range. Interquartile merupakan range diantara kuartil pertama(25%) dan kuartil ketiga(75%)."""

# Convert 'Date' to datetime if it's not already
df['Date'] = pd.to_datetime(df['Date'])

# Exclude non-numeric columns before calculating quantiles
numeric_df = df.select_dtypes(include=np.number)

Q1 = numeric_df.quantile(.25)
Q3 = numeric_df.quantile(.75)

IQR = Q3 - Q1

bot_treshold = Q1 - 1.5 * IQR
top_treshold = Q3 + 1.5 * IQR

# Apply the filtering on the original dataframe
df = df[~((numeric_df < bot_treshold) | (numeric_df > top_treshold)).any(axis=1)]
df.shape

"""Jika di lihat dari visualisasi data dibawah. Fitur Close pada sumbu y memiliki korelasi dengan data pada fitur High, Low, Open"""

sns.pairplot(df, diag_kind = 'kde')
plt.show()

"""##**Data Preparation**##

Kolom data seperti ( Date, vol, adj close) tidak diperlukan untuk pelatihan, karena data tersebut akan mengganggu model dalam mempelajari data. Karena isi dari data tersebut tidak memiliki value yang berarti untuk dipelajari oleh model. Lalu, mengubah nama kolom High, Low, Open, Close menjadi nama kolom yang dapat lebih dipahami.
"""

unused_columns = ['Date', 'Volume', 'Adj Close']
renamed_columns = {'High': 'High_Price', 'Low': 'Low_Price',
                   'Open': 'Open_Price', 'Close': 'Close_Price'}

df.drop(unused_columns, axis=1, inplace=True)
df.rename(columns=renamed_columns, inplace=True)
df

"""##**Split dataset**##

Membagi dataset menjadi data training dan data testing
"""

x = df.drop(['Close_Price'], axis=1).values
y = df['Close_Price'].values

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=1)

print(f'Total # of sample in whole dataset: {len(x)}')
print(f'Total # of sample in train dataset: {len(x_train)}')
print(f'Total # of sample in test dataset: {len(x_test)}')

"""##**Normalisasi data**##

MinMaxScaler mentransformasikan fitur dengan menskalakan setiap fitur ke rentang tertentu. Library ini menskalakan dan mentransformasikan setiap fitur secara individual sehingga berada dalam rentang yang diberikan pada set pelatihan, pada library ini memiliki range default antara nol dan satu.
"""

scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)

models = pd.DataFrame(columns=['train_mse', 'test_mse'],
                      index=['KNN', 'RandomForest', 'SVR', 'XGboost'])

"""##**Model Development**##

Untuk melakukan tuning hyperparameter pada proyek ini menggunakan teknik Grid search. Grid search memungkinkan untuk menguji beberapa parameter sekaligus pada sebuah model.
"""

svr = SVR()
parameters = {
    'kernel': ['rbf'],
    'C':     [1000, 10000, 100000],
    'gamma': [0.3, 0.03, 0.003]
}

svr_search = GridSearchCV(
    svr,
    parameters,
    cv=5,
    verbose=1,
    n_jobs=6,
)

svr_search.fit(x_train, y_train)
svr_best_params = svr_search.best_params_

knn = KNeighborsRegressor()
parameters =  {
    'n_neighbors': range(1, 25),
}

knn_search = GridSearchCV(
  knn,
  parameters,
  cv=5,
  verbose=1,
  n_jobs=6,
)

knn_search.fit(x_train, y_train)
knn_best_params = knn_search.best_params_

rf = RandomForestRegressor()
parameters =  {
    'n_estimators': range(1, 10),
    'max_depth': [16, 32, 64],
}

rf_search = GridSearchCV(
  rf,
  parameters,
  cv=5,
  verbose=1,
  n_jobs=6,
)
rf_search.fit(x_train, y_train)
rf_best_params = rf_search.best_params_

xgb_model = xgb.XGBRegressor()
parameters = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}
xgb_search = GridSearchCV(
    xgb_model,
    parameters,
    cv=5,
    verbose=1,
    n_jobs=6,
)
xgb_search.fit(x_train, y_train)
xgb_best_params = xgb_search.best_params_

"""##**Training Model**##

**Support Vector Machine**
"""

svr = SVR(
  C=svr_best_params["C"],
  gamma=svr_best_params["gamma"],
  kernel=svr_best_params['kernel']
)
svr.fit(x_train, y_train)

"""**K-Nearest Neighbours**"""

knn = KNeighborsRegressor(n_neighbors=knn_best_params["n_neighbors"])
knn.fit(x_train, y_train)

"""**Random Forest**"""

rf = RandomForestRegressor(
  n_estimators=rf_best_params["n_estimators"],
  max_depth=rf_best_params["max_depth"]
)
rf.fit(x_train, y_train)

"""**Gradient Boosting Machines (XGBoost)**"""

xgb_model = xgb.XGBRegressor(
    **xgb_best_params
)
xgb_model.fit(x_train, y_train)

"""##**Model Evaluation**##"""

x_test = scaler.transform(x_test)

model_dict = {'KNN': knn, 'RandomForest': rf, 'SVR': svr, 'XGboost' : xgb_model}

for name, model in model_dict.items():
  models.loc[name, 'train_mse'] = mean_squared_error(
    y_true=y_train,
    y_pred=model.predict(x_train)
  )
  models.loc[name, 'test_mse'] = mean_squared_error(
    y_true=y_test,
    y_pred=model.predict(x_test)
  )

models

fig, ax = plt.subplots()
models.sort_values(by='test_mse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

test_data = x_test.copy()
predictions = {'y_true':y_test}
for name, model in model_dict.items():
  predictions['prediction_' + name] = model.predict(test_data)

predictions = pd.DataFrame(predictions)
predictions

predictions = predictions.tail(10)
predictions.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()